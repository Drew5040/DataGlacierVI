{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10d565c",
   "metadata": {},
   "source": [
    "#### Text Lowercasing:\n",
    "\n",
    "Convert all text to lowercase to ensure uniformity. This prevents the model from treating \"HATE\" and \"hate\" as different words.\n",
    "\n",
    "#### Removing Special Characters and Punctuation:\n",
    "\n",
    "Remove punctuation marks, special characters, and symbols, as they don't typically provide meaningful information for hate speech detection.\n",
    "\n",
    "#### Removing URLs and User Mentions:\n",
    "\n",
    "Remove URLs and user mentions (e.g., @username) from the text, as they are not relevant to the analysis.\n",
    "\n",
    "#### Removing Numbers:\n",
    "\n",
    "Remove numerical digits or replace them with a placeholder token if numbers don't convey important information.\n",
    "\n",
    "#### Tokenization:\n",
    "\n",
    "Tokenize the text into words or subword units. Tokenization breaks the text into smaller units, making it easier for the NLP model to process.\n",
    "\n",
    "#### Removing Stop Words:\n",
    "\n",
    "Depending on the specific analysis, you may choose to remove common stop words (e.g., \"the,\" \"and,\" \"in\") to reduce noise. However, be cautious when removing stop words, as they can be relevant in some hate speech contexts.\n",
    "\n",
    "#### Stemming or Lemmatization (Optional):\n",
    "\n",
    "Apply stemming or lemmatization to reduce words to their base forms. This can help normalize the text and reduce feature dimensionality. Be mindful of the potential loss of context when applying these techniques.\n",
    "\n",
    "#### Handling Emojis and Emoticons:\n",
    "\n",
    "Decide whether to keep or remove emojis and emoticons. Some hate speech may include offensive symbols or characters, so consider their importance to the analysis.\n",
    "Handling Abbreviations and Acronyms:\n",
    "\n",
    "Expand or normalize common abbreviations and acronyms to their full forms (e.g., \"lol\" to \"laugh out loud\").\n",
    "\n",
    "#### Spelling Correction (Optional):\n",
    "\n",
    "Depending on the quality of the data, you may choose to apply spelling correction to address typos and misspellings.\n",
    "\n",
    "#### Removing or Masking Sensitive Information (Optional):\n",
    "\n",
    "If the data contains sensitive or personally identifiable information (PII), consider removing or masking it to protect privacy and comply with data regulations.\n",
    "\n",
    "#### Filtering Non-Textual Content (Optional):\n",
    "\n",
    "Depending on your analysis, you may need to filter out non-textual content such as images or videos associated with tweets.\n",
    "**Handling Imbalanced Data (if applicable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from spellchecker import WordFrequency\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the maximum width for column display to a large value (e.g., 200 characters)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4384992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory\n",
    "print(os.getcwd())\n",
    "os.chdir('C://Users/andre/Job Portfolio Projects/DataGlacierVI/sentiment.analysis/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftt = pd.read_csv('test_tweets.csv')\n",
    "dftrt = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dftt.head())\n",
    "display(dftt.info())\n",
    "display(dftt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dftrt.head())\n",
    "display(dftrt.info())\n",
    "display(dftrt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of original dataset that test data is\n",
    "dftt.shape[0]/(dftt.shape[0] + dftrt.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1665929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average tweet length\n",
    "\n",
    "def avg_len_tweet(colname):\n",
    "    \n",
    "    # Create empty list\n",
    "    lst = []\n",
    "    \n",
    "    # For loop to iterate through every tweet\n",
    "    for tweet in dftrt[colname]:\n",
    "        \n",
    "        # Get len of every string\n",
    "        length = len(tweet)\n",
    "        \n",
    "        # Append the length to a list\n",
    "        lst.append(length)\n",
    "        \n",
    "    # Sum all elements in list\n",
    "    len_sum = sum(lst)\n",
    "    \n",
    "    # Divide 'len_sum' by length of list\n",
    "    print('Average tweet length: %d characters' % (len_sum/len(lst)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e993f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b55802",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len_tweet('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54477803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "   \n",
    "    # Convert to Lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Remove user mentions like: '@username'\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove special characters and punctuation (except for spaces)\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'[0-9]', '', tweet)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets of punctuation, special characters, url's, user mentions, numbers, and extra spaces\n",
    "\n",
    "dftrt['clean_tweet'] = dftrt['tweet'].apply(clean_tweet)\n",
    "dftrt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d51b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftt['clean_tweet'] = dftt['tweet'].apply(clean_tweet)\n",
    "dftt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f52db",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2864e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load and preprocess training data\n",
    "def preprocess_data(csv_file, tokenizer):\n",
    "    # Load .csv\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Clean the tweets of the training set\n",
    "    df['clean_tweet'] = df['tweet'].apply(clean_tweet) \n",
    "    \n",
    "    # Create an instance of the tokenizer\n",
    "    tokenizer = tokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    tokenized_texts = []\n",
    "\n",
    "    for text in df['clean_tweet']:\n",
    "        try:\n",
    "            # Convert the token IDs to string tokens\n",
    "            tokens = [str(token_id) for token_id in tokenizer.encode(str(text), add_special_tokens=True, is_split_into_words=True)]\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting tokens to IDs: {e}\")\n",
    "            print(f\"Problematic text: {text}\")\n",
    "            continue  # Skip this text and move on to the next\n",
    "\n",
    "        tokenized_texts.append(tokens)\n",
    "\n",
    "    # Set max length of tensors\n",
    "    max_len = 110  \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for tokens in tokenized_texts:\n",
    "        # Truncate if the sequence is longer than max_len\n",
    "        if len(tokens) > max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "        \n",
    "        # Tokenize and pad input_ids\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)  \n",
    "        input_id += [0] * (max_len - len(input_id))\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    # Convert the lists to PyTorch tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    if csv_file == 'train_tweets.csv':\n",
    "        # Create a tensor for target labels\n",
    "        labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    \n",
    "\n",
    "# Load and preprocess testing data (similar to training data)\n",
    "input_ids_train, attention_masks_train, y_train = preprocess_data('train_tweets.csv', XLNetTokenizer)\n",
    "input_ids_test, attention_masks_test = preprocess_data('test_tweets.csv', XLNetTokenizer)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 110\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_data = TensorDataset(input_ids_test, attention_masks_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Define the XLNet-based model\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)  \n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_masks = batch\n",
    "        input_ids, attention_masks = input_ids.to(device), attention_masks.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# y_test = []  # Replace this with your test labels\n",
    "# y_pred = []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids, attention_masks, labels = batch\n",
    "#         input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_masks)\n",
    "#         logits = outputs.logits\n",
    "#         predictions = torch.argmax(logits, dim=1)\n",
    "#         y_pred.extend(predictions.cpu().numpy())\n",
    "#         y_test.extend(labels.cpu().numpy())  # Replace this with your test labels\n",
    "\n",
    "# # Calculate accuracy and print classification report\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c51e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d622506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba481702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8684d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d1acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea239f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803de2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2ca03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b6ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815f789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a026f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"This is a sample tweet.\",\n",
    "    \"Another tweet to tokenize.\"\n",
    "]\n",
    "\n",
    "tokenized_tweets = [tokenizer.encode(tweet, add_special_tokens=True) for tweet in tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e70d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrt['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93673add",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrt[dftrt['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2f8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1702c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379f17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6828b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cf07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c52b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e332d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c58283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c5c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57d660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7bcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e93a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de3257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a54c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688021c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'XXXXXXXXXXXXX'\n",
    "    consumer_secret = 'XXXXXXXXXXXXX'\n",
    "    access_token = 'XXXXXXXXXXXXXX'\n",
    "    access_token_secret = 'XXXXXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create OAuth object\n",
    "    self.auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    # Set access_token and secret\n",
    "    self.auth.set_access_token(access_token, access_token_secret)\n",
    "    # Create tweepy API object to fetch tweets\n",
    "    self.api = tweepy.API(self.auth)\n",
    "except:\n",
    "    print('Error: Authentication Failed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.search('anything_you_want_to_search')\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = TextBlob(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa6692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae1716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e73a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7af63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c76fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
