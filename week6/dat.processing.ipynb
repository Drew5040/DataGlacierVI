{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfdd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import subprocess\n",
    "import yaml\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import psutil\n",
    "import gc\n",
    "import graphviz\n",
    "import objgraph\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import ray\n",
    "import modin.pandas as mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir('C:\\\\Users\\\\andre\\\\Job Portfolio Projects\\\\DataGlacierVI\\\\data.processing')\n",
    "print(cwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd5ec7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caf51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to keep track of df objects in memory\n",
    "def dfs():\n",
    "    \"\"\"\n",
    "    List all Pandas DataFrame objects currently in memory.\n",
    "\n",
    "    This function lists all Pandas DataFrame objects present in the global namespace.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of DataFrame objects.\n",
    "    \"\"\"\n",
    "    dataframes = [var for var in globals() if isinstance(globals()[var], pd.DataFrame)]\n",
    "    print(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6083ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_ram():\n",
    "    \"\"\"\n",
    "    Get information about the system's RAM (Random Access Memory) usage.\n",
    "\n",
    "    Returns:\n",
    "        psutil._common.svmem: A named tuple representing RAM usage statistics.\n",
    "    \"\"\"\n",
    "    memory_usage = psutil.virtual_memory()\n",
    "    return memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ram_graph(working_dir, dot_name, objects):\n",
    "    \"\"\"\n",
    "    Generate a reference graph of Python objects and save it as a PNG image.\n",
    "\n",
    "    Parameters:\n",
    "        working_dir (str): The directory where the DOT file and PNG image will be saved.\n",
    "        dot_name (str): The base name for the DOT and PNG files (without extensions).\n",
    "        objects: The Python objects for which references will be graphed.\n",
    "\n",
    "    Returns:\n",
    "        graphviz.files.Source: A Graphviz Source object representing the rendered reference graph.\n",
    "    \"\"\"\n",
    "    # Specify the full file path where the DOT file should be saved\n",
    "    dot_file_path = f'{working_dir}/{dot_name}.dot'\n",
    "    \n",
    "    # Generate the object references graph and save it as a DOT file\n",
    "    objgraph.show_refs(objects, filename=dot_file_path)\n",
    "    \n",
    "    # Specify the full file path where the PNG image should be saved\n",
    "    png_file_path = f'{working_dir}/{dot_name}.png'\n",
    "    \n",
    "    # Use graphviz to render the DOT file as a PNG image\n",
    "    graph = graphviz.Source.from_file(dot_file_path, format='png')\n",
    "    \n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0421788",
   "metadata": {},
   "source": [
    "## Create Mock Dataset with Special Characters, Strings, Integers, & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263eecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_value():\n",
    "    \"\"\"\n",
    "    Generate a random value with various data types and potential missing values.\n",
    "\n",
    "    Returns:\n",
    "        str, int, float, or None: A randomly generated value, which can be a string, integer, float,\n",
    "        or None (representing a missing value).\n",
    "\n",
    "    Description:\n",
    "        This function generates random values with different data types and the possibility of missing values.\n",
    "        - 10% chance of returning a missing value (None).\n",
    "        - 20% chance of returning a string consisting of 5 random special characters.\n",
    "        - 50% chance of returning a string consisting of 10 random alphanumeric characters.\n",
    "        - 20% chance of returning either a random integer between 1 and 1000 (inclusive) or\n",
    "          a random float between 0.1 and 1000.0 (inclusive).\n",
    "\n",
    "    Example:\n",
    "        Possible outputs:\n",
    "        - 'ABc!@#' (string with special characters)\n",
    "        - 123 (integer)\n",
    "        - 456.789 (float)\n",
    "        - None (missing value)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 10% chance of missing value\n",
    "    if random.random() < 0.1:  \n",
    "        return np.nan\n",
    "    # 20% chance of special character\n",
    "    elif random.random() < 0.2:  \n",
    "        return ''.join(random.choice(string.punctuation) for _ in range(5))\n",
    "    # 50% chance of string\n",
    "    elif random.random() < 0.5:  \n",
    "        return ''.join(random.choice(string.ascii_letters) for _ in range(10))\n",
    "     # 20% chance of number (integer or float)\n",
    "    else: \n",
    "        return random.choice([random.randint(1, 1000), random.uniform(0.1, 1000.0)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_large_dataframe():\n",
    "    \"\"\"\n",
    "    Generate a large Pandas DataFrame with random data and save it to a CSV file when it exceeds 2GB in size.\n",
    "\n",
    "    Description:\n",
    "        This function generates random data and creates a Pandas DataFrame. It keeps adding rows to the DataFrame\n",
    "        until its size exceeds 2GB. Once the size limit is reached, the DataFrame is saved to a CSV file\n",
    "        with a filename indicating the size of the dataset in millions of rows (e.g., 'mock_dataset_6M.csv').\n",
    "\n",
    "    Note:\n",
    "        The function uses the `generate_random_value` function to create random data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    num_columns = 25\n",
    "    num_rows = 6000000\n",
    "\n",
    "    while True:\n",
    "        # Generate dict substructure for a Pandas dataframe\n",
    "        for i in range(num_columns):\n",
    "            column_name = f'column_{i+1}'\n",
    "            data[column_name] = [generate_random_value() for _ in range(num_rows)]\n",
    "\n",
    "        # Create a Pandas DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        df_size_bytes = df.memory_usage(index=True).sum()\n",
    "\n",
    "        # Check size of dataframe: if > than 2GB, save the file\n",
    "        if df_size_bytes > 2 * 1073741824: \n",
    "            filename = f'mock_dataset_{num_rows//1000000}M.csv'\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Saved DataFrame to {filename}\")\n",
    "            break  \n",
    "        # Else, add another 2000000 rows to the df\n",
    "        else:\n",
    "            del df \n",
    "            num_rows += 2000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aa733",
   "metadata": {},
   "source": [
    "## Decide Library to Use to Speed Up Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Pandas\n",
    "\n",
    "def load_csv_with_pandas(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Pandas and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Pandas (in seconds).\n",
    "    \"\"\"\n",
    "    start_time1 = time.time()\n",
    "    df = pd.read_csv(file_path)\n",
    "    end_time1 = time.time() - start_time1\n",
    "    return end_time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Dask\n",
    "\n",
    "def load_csv_with_dask(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Dask and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Dask (in seconds).\n",
    "    \"\"\"\n",
    "    start_time2 = time.time()\n",
    "    ddf = dd.read_csv(file_path)\n",
    "    end_time2 = time.time() - start_time2\n",
    "    return end_time2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560febbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Ray\n",
    "\n",
    "ray.init(runtime_env={'env_vars': {'__MODIN_AUTOIMPORT_PANDAS__': '1'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17183a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## > Modin\n",
    "\n",
    "def load_csv_with_modin(file_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file using Modin and measure the loading time.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: The time taken to load the CSV file using Modin (in seconds).\n",
    "    \"\"\"\n",
    "    start_time3 = time.time()\n",
    "    mdf = mpd.read_csv(file_path)\n",
    "    end_time3 = time.time() - start_time3\n",
    "    return end_time3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d70d0d",
   "metadata": {},
   "source": [
    "# Automated Data Validation Pipeline: Using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d931696",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_config_file(filepath):\n",
    "    \n",
    "     \"\"\"\n",
    "    Read and parse a YAML configuration file.\n",
    "\n",
    "    Parameters:\n",
    "        filepath (str): The path to the YAML configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the parsed configuration data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open specified filepath\n",
    "    with open(filepath, 'r') as datacreek:\n",
    "        # Try-Catch for YAMLError\n",
    "        try:\n",
    "            return yaml.safe_load(datacreek)\n",
    "        except yaml.YAMLError as exc:\n",
    "            # Logging library error sent to 'stdout'\n",
    "            logging.error(exc)\n",
    "            \n",
    "\n",
    "def replacer(string, char):\n",
    "    \"\"\"\n",
    "    Replace consecutive instances of a character in a string with a single instance.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): The input string.\n",
    "        char (str): The character to be replaced.\n",
    "\n",
    "    Returns:\n",
    "        str: The input string with consecutive instances of the character replaced.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex pattern for 2 or more instances\n",
    "    pattern = char + '{2,}'\n",
    "    \n",
    "    # Sub function to replace specified char\n",
    "    string = re.sub(pattern, char, string)\n",
    "    \n",
    "    # Returns the string with replaced char\n",
    "    return string\n",
    "\n",
    "def col_header_val(df, table_config):\n",
    "    \"\"\"\n",
    "    Validate and standardize column names in a DataFrame based on a table configuration.\n",
    "\n",
    "    Parameters:\n",
    "        df (dd.DataFrame): The Dask DataFrame to be validated.\n",
    "        table_config (dict): The table configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if validation passes, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert all strings to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    " \n",
    "    # Replace all whitespce at the start of column names\n",
    "    df.columns = df.columns.str.replace('[^\\w]', '_', regex=True)\n",
    "    \n",
    "    # Removes underscores from beginning & end of column names\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "   \n",
    "    # Replaces 2 or more consecutive underscores with a single underscore\n",
    "    df.columns = list(map(lambda x: replacer(x, '_'), list(df.columns)))\n",
    "    \n",
    "    # Converts expected col_names for 'table_config' to ensure case insensitivity during comaparison\n",
    "    expected_col = list(map(lambda x: x.lower(), table_config['columns']))\n",
    "   \n",
    "  \n",
    "    # Converts all column names in df once again. Ensures case insensitivity when comparing with expected col_names\n",
    "    df.columns = list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    \n",
    "    # Sort the DataFrame by multiple columns\n",
    "    df = df[table_config['columns']]\n",
    "\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col) == list(df.columns):\n",
    "        print('column namd and column length validation passed')\n",
    "        return True\n",
    "\n",
    "    # If the above is false, then we check what the differences are between df.col and exp_col and print them\n",
    "    else:\n",
    "        print('column name and column length validation failed')\n",
    "        # Uses set operations for taking the difference between df.col and exp_col\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print('The following YAML columns are not in the YAML file', mismatched_columns_file )\n",
    "        # Uses set operations to check diff between exp_col and df_col\n",
    "        missing_yaml_file = list(set(expected_col).difference(df.columns))\n",
    "        print('The following YAML columns are not in the uploaded file', missing_yaml_file)\n",
    "        # log results\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_yaml_config(file_type, file_name, table_name, columns, in_del, out_del):\n",
    "    \"\"\"\n",
    "    Generate a YAML configuration file for data processing and save it.\n",
    "\n",
    "    Parameters:\n",
    "        file_type (str): The type of the data file (e.g., 'csv', 'parquet').\n",
    "        file_name (str): The name of the data file.\n",
    "        table_name (str): The name of the data processing table.\n",
    "        columns (list): A list of column names.\n",
    "        in_del (str): The inbound delimiter character.\n",
    "        out_del (str): The outbound delimiter character.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated file_name, file_type, and table_name.\n",
    "    \"\"\"\n",
    "    # Create a configuration dictionary\n",
    "    config_data = {\n",
    "        'file_type': file_type,\n",
    "        'dataset_name': 'testfile',\n",
    "        'file_name': file_name,\n",
    "        'table_name': table_name,\n",
    "        'inbound_delimiter': in_del,\n",
    "        'outbound_delimiter': out_del,\n",
    "        'skip_leading_rows': 1,\n",
    "        'columns': columns,\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to YAML format\n",
    "    yaml_config = yaml.dump(config_data, default_flow_style=False)\n",
    "\n",
    "    # Save the YAML configuration to a file\n",
    "    with open(f'{table_name}_config.yaml', 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_config)\n",
    "        \n",
    "    return file_name, file_type, table_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_file(file_type, source_file):\n",
    "    \"\"\"\n",
    "    Read data from a file into a Dask DataFrame based on the file type.\n",
    "\n",
    "    Parameters:\n",
    "        file_type (str): The type of the data file (e.g., 'csv', 'excel', 'parquet').\n",
    "        source_file (str): The path to the source data file.\n",
    "\n",
    "    Returns:\n",
    "        dd.DataFrame: A Dask DataFrame containing the data from the file.\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        return dd.read_csv(source_file)\n",
    "    elif file_type == 'excel':\n",
    "        return dd.read_excel(source_file)\n",
    "    elif file_type == 'parquet':\n",
    "        return dd.read_parquet(source_file)\n",
    "    # Add more file types as needed...\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def save_file(df, file_type, target_path):\n",
    "    \"\"\"\n",
    "    Save a Dask DataFrame to a file with the specified file type.\n",
    "\n",
    "    Parameters:\n",
    "        df (dd.DataFrame): The Dask DataFrame to be saved.\n",
    "        file_type (str): The file type (e.g., 'csv', 'parquet', 'excel').\n",
    "        target_path (str): The path where the file should be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was saved successfully or already exists, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_type == 'csv':\n",
    "            if os.path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_csv(target_path, single_file=True)\n",
    "        elif file_type == 'parquet':\n",
    "            if os.path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_parquet(target_path)\n",
    "        elif file_type == 'excel':\n",
    "            if os.path.exists(target_path):\n",
    "                print(f\"File already exists: {target_path}\")\n",
    "            else:\n",
    "                df.to_excel(target_path, index=False)\n",
    "        # Add more file types as needed\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {file_type}\")\n",
    "            return False\n",
    "\n",
    "        print(f\"File saved successfully: {target_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0151444",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e21c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### > Main script\n",
    "\n",
    "\n",
    "# Code wasnt heavily tested for bugs due to a lack of time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    import testutility as util\n",
    "    \n",
    "    # List of file configurations\n",
    "    file_configurations = [\n",
    "\n",
    "        {\n",
    "            'file_name': 'file1',\n",
    "            'file_type': 'csv',\n",
    "            'table_name': 'table1',\n",
    "            'columns': ['col1', 'col2', 'col3'],\n",
    "            'in_del': ',',\n",
    "            'out_del': '|'\n",
    "        },\n",
    "        {\n",
    "            'file_name': 'file2',\n",
    "            'file_type': 'excel',\n",
    "            'table_name': 'table2',\n",
    "            'columns': ['colA', 'colB', 'colC'],\n",
    "            'in_del': '\\t',\n",
    "            'out_del': ','\n",
    "        }\n",
    "\n",
    "        # Add more configurations as needed\n",
    "    ]\n",
    "    \n",
    "    # Create a loop to process each configuration\n",
    "    for config in file_configurations:\n",
    "        file_name, file_type, table_name = generate_yaml_config(\n",
    "    \n",
    "                    config['file_type'],\n",
    "                    config['file_name'],\n",
    "                    config['table_name'],\n",
    "                    config['columns'],\n",
    "                    config['in_del'],\n",
    "                    config['out_del']\n",
    "        )\n",
    "    \n",
    "        # Read config file\n",
    "        config_data = util.read_config_file(file_name)\n",
    "\n",
    "        # Read file using config file\n",
    "        file_type = config_data['file_type']\n",
    "        source_file = './' + config_data['file_name'] + f'.{file_type}'\n",
    "\n",
    "        # Read the file based on its type\n",
    "        ddf = util.read_file(file_type, source_file)\n",
    "\n",
    "        # Perform column validation\n",
    "        result = util.col_header_val(ddf, config_data)\n",
    "\n",
    "        if result:\n",
    "            # Define the target file path and file type\n",
    "            target_file_path = f'completed_{config_data[\"file_name\"]}.{config_data[\"file_type\"]}'\n",
    "\n",
    "            # Save the file using the save_file function\n",
    "            save_result = save_file(ddf, config_data[\"file_type\"], target_file_path)\n",
    "\n",
    "            if save_result:\n",
    "                print(f'Successfully processed and saved: {config_data[\"file_name\"]}')\n",
    "            else:\n",
    "                print(f'Error saving the file for: {config_data[\"file_name\"]}')\n",
    "        else:\n",
    "            print(f'Validation failed for: {config_data[\"file_name\"]}')\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
