{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ead087",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38173349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd734622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have the rest already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import contractions\n",
    "import html\n",
    "import translate\n",
    "import sentencepiece\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import tensorflow\n",
    "import multiprocessing\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from translate import Translator\n",
    "from emoji import demojize\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Set the maximum width for column display to a large value (e.g., 200 characters)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4384992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Job Portfolio Projects\\DataGlacierVI\\sentiment.analysis\n",
      "C:\\Users\\andre\\Job Portfolio Projects\\DataGlacierVI\\sentiment.analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Change directory\n",
    "print(os.getcwd())\n",
    "os.chdir('C://Users/andre/Job Portfolio Projects/DataGlacierVI/sentiment.analysis/')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2e2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dftrt = pd.read_csv('train_tweets.csv')\n",
    "# dftt = pd.read_csv('test_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dftt.head())\n",
    "display(dftt.info())\n",
    "display(dftt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dftrt.head())\n",
    "display(dftrt.info())\n",
    "display(dftrt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of original dataset that test data is\n",
    "dftt.shape[0]/(dftt.shape[0] + dftrt.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1665929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average tweet length in characters\n",
    "\n",
    "def avg_num_chars(df, colname):\n",
    "    \n",
    "    # Create empty list\n",
    "    lst = []\n",
    "    # For loop to iterate through every tweet\n",
    "    for tweet in df[colname]:\n",
    "        \n",
    "        # Get len of every string\n",
    "        length = len(tweet)\n",
    "        \n",
    "        # Append the length to a list\n",
    "        lst.append(length)\n",
    "\n",
    "    # Sum all elements in list\n",
    "    len_sum = sum(lst)\n",
    "    \n",
    "    # Divide 'len_sum' by length of list\n",
    "    print('Average tweet length: %d characters' % (len_sum/len(lst)))\n",
    "    print('With a standard deviation of: %d characters per tweet' % (np.std(lst)), '\\n')\n",
    "    # Return result\n",
    "    return len_sum/len(lst), np.std(lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1eb7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_num_wrds(df, colname):\n",
    "    \n",
    "    # Lst to capture lengths of wrd_lsts\n",
    "    lst = []\n",
    "    \n",
    "    # Sum variable\n",
    "    word_sum = 0\n",
    "    \n",
    "    # Loop for counting words\n",
    "    for tweet in df[colname]:\n",
    "        \n",
    "        # Split tweet according to white space\n",
    "        word_lst = tweet.split()\n",
    "        \n",
    "        # Find length of split lst\n",
    "        num_wrds = len(word_lst)\n",
    "        \n",
    "        # Append lengths of num_wrds in each lst\n",
    "        lst.append(num_wrds)\n",
    "        \n",
    "        # Sum the wrds of the tweet\n",
    "        word_sum += num_wrds\n",
    "        \n",
    "    # Divide by the number of tweets to get average num wrds\n",
    "    avg_wrds_in_tweet = word_sum / len(df[colname])\n",
    "    \n",
    "    # Print results\n",
    "    print('Average number of words per tweet: %d' % (avg_wrds_in_tweet))\n",
    "    print('Standard deviation of the number of words: %d' % (np.std(lst)), '\\n')\n",
    "    \n",
    "    # Return result\n",
    "    return avg_wrds_in_tweet, np.std(lst)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of tweets in training set (pre-cleaning)\n",
    "num_tweets = len(dftrt)\n",
    "num_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d85dde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 84 characters\n",
      "With a standard deviation of: 29 characters per tweet \n",
      "\n",
      "Average number of words per tweet: 13\n",
      "Standard deviation of the number of words: 5 \n",
      "\n",
      "Average length of a word in a tweet: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Averages of number of chars, words, in tweets. Plus standard deviations (pre-cleaning)\n",
    "\n",
    "avg_and_stdvs = avg_num_chars(dftrt, 'tweet')[0]/avg_num_wrds(dftrt, 'tweet')[0]\n",
    "print('Average length of a word in a tweet: %d' % (avg_and_stdvs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54477803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "    # Instantiate the translator\n",
    "    translator = Translator(to_lang='en')\n",
    "    \n",
    "    # Translate to english\n",
    "    tweet = translator.translate(tweet)\n",
    "   \n",
    "    # Convert to Lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Fix contractions\n",
    "    tweet = contractions.fix(tweet)\n",
    "    \n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    # Remove user mentions like: '@username'\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove special characters and punctuation (except for spaces)\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'[0-9]', '', tweet)\n",
    "    \n",
    "    # Remove all non-ascii chars\n",
    "    tweet = ''.join(char for char in tweet if ord(char) < 128)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    \n",
    "    # Remove html\n",
    "    tweet = html.unescape(tweet)\n",
    "    \n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f637cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nulls pre-cleaning\n",
    "rows_with_null = dftrt.isnull().sum()\n",
    "rows_with_null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3bed67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when a father is dysfunctional and is so selfish he drags his kids into his dysfunction run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for lyft credit i cannot use because they do not offer wheelchair vans in pdx disapointed getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model i love you take with you all the time in you are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run   \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2                                                                                                         bihday your majesty   \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦     \n",
       "4                                                                                      factsguide: society now    #motivation   \n",
       "\n",
       "                                                                                                   clean_tweet  \n",
       "0                  when a father is dysfunctional and is so selfish he drags his kids into his dysfunction run  \n",
       "1  thanks for lyft credit i cannot use because they do not offer wheelchair vans in pdx disapointed getthanked  \n",
       "2                                                                                          bihday your majesty  \n",
       "3                                                       model i love you take with you all the time in you are  \n",
       "4                                                                            factsguide society now motivation  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Cleans tweets of punctuation, special characters, url's, user mentions, numbers, extra spaces, emoji's, html\n",
    "\n",
    "dftrt['clean_tweet'] = dftrt['tweet'].apply(clean_tweet)\n",
    "dftrt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7464d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 70 characters\n",
      "With a standard deviation of: 28 characters per tweet \n",
      "\n",
      "Average number of words per tweet: 12\n",
      "Standard deviation of the number of words: 5 \n",
      "\n",
      "Average length of a word in a tweet: 5\n"
     ]
    }
   ],
   "source": [
    "# Avg chars, words, and avg lenght of word in tweet (post-cleaning)\n",
    "\n",
    "avg_and_stdvs = avg_num_chars(dftrt, 'clean_tweet')[0]/avg_num_wrds(dftrt, 'clean_tweet')[0]\n",
    "print('Average length of a word in a tweet: %d' % (avg_and_stdvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nulls_after_cleaning = dftrt.isnull().sum()\n",
    "nulls_after_cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_batch(texts):\n",
    "    # Spell check a batch of texts\n",
    "    return [spell.correction(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf735d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spell checker\n",
    "def spell_check(tweet):\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = tweet.split()\n",
    "    # Track original and corrected words\n",
    "    original_tweet = []\n",
    "    corrected_tweet = []\n",
    "    \n",
    "    for word in words:\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is not None: \n",
    "            corrected_tweet.append(corrected_word)\n",
    "        else:\n",
    "            corrected_tweet.append(word)  # Keep the original word if no correction found\n",
    "        original_tweet.append(word)\n",
    "        \n",
    "    # Join the corrected words back into a sentence\n",
    "    corrected_tweet_text = ' '.join(corrected_tweet)\n",
    "    original_tweet_text = ' '.join(original_tweet)\n",
    "    \n",
    "    \n",
    "    return original_tweet_text, corrected_tweet_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194942af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the spell checker and word frequency objects\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Split your dataset into batches\n",
    "batch_size = 100\n",
    "\n",
    "batch = [dftrt['clean_tweet'][i:i+batch_size] for i in range(0, len(dftrt), batch_size)]\n",
    "\n",
    "# Initialize a multiprocessing pool\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "\n",
    "# \n",
    "corrected_batch = pool.map(spell_check_batch, batch)\n",
    "\n",
    "# Combine the results into a single list\n",
    "corrected_tweet = [text for batch in corrected_batches for text in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "print(spell.correction('sme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfabbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrt['spell_checked_tweet'] = dftrt['clean_tweet'].apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "na_after_cleaning = dftrt.isna().sum()\n",
    "na_after_cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### > Splitting the Data: Using the 'train_tweets' for training, val, & testing\n",
    "\n",
    "# Shuffle the training, validation, & testing data\n",
    "shuffled_data = dftrt.sample(frac=1, random_state=42)\n",
    "\n",
    "# Instantiate ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate size of each dataset\n",
    "train_size = int(len(shuffled_data) * train_ratio)\n",
    "val_size = int(len(shuffled_data) * val_ratio)\n",
    "test_size = int(len(shuffled_data) * test_ratio)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data = shuffled_data[:train_size]\n",
    "val_data = shuffled_data[train_size:(train_size + val_size)]\n",
    "test_data = shuffled_data[(train_size + val_size):(train_size + val_size + test_size)]\n",
    "\n",
    "# Ensure order is preserved within each set\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2864e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess training data\n",
    "def preprocess_data(df, tokenizer, max_len=110, apply_smote=False, test_data=False):\n",
    "\n",
    "    # Clean the tweets of the training set\n",
    "    df['clean_tweet'] = df['tweet'].apply(clean_tweet)\n",
    "\n",
    "    # Create an instance of the tokenizer\n",
    "    tokenizer_instance = tokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "    # List to hold text tokens: [\"Hello\", \",\", \"world\", \"!\", \"[SEP]\"]\n",
    "    tokenized_texts = []\n",
    "\n",
    "    # Tokenize the tweets: try-block to identify any problem tokens\n",
    "    for tweet in df['clean_tweet']:\n",
    "\n",
    "        try:\n",
    "\n",
    "          # Convert the token IDs to strings: special tokens = [SEP] etc..\n",
    "          # str(token_id): casting int to str\n",
    "          # is_split_into_words: splits tokens into words or subwords\n",
    "          # Ex: [[CLS], 'un', 'happiness', [CLS]] <- 'unhappiness' split\n",
    "\n",
    "            tokens = [str(token_id) for token_id in tokenizer_instance.encode(str(tweet), add_special_tokens=True, is_split_into_words=True)]\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error converting tokens to IDs: {e}\")\n",
    "            print(f\"Problematic text: {tweet}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Append the tokens to list: [\"Hello\", \",\", \"world\", \"!\", \"[SEP]\"]\n",
    "        tokenized_texts.append(tokens)\n",
    "\n",
    "\n",
    "    # List to hold input ID's: [1045, 2293, 3019, 3601, 2653, 6360]\n",
    "    input_ids_list = []\n",
    "\n",
    "    # List to hold attention masks: [1, 1, 1, 0, 0, 0]\n",
    "    attention_masks = []\n",
    "\n",
    "    # Truncate text sequences if necessary\n",
    "    for tokens in tokenized_texts:\n",
    "\n",
    "        # Tokenize and pad input_ids: each token gets mapped to a unique int ID\n",
    "        # if sequence is < 100, then 0's to fill the missing elements\n",
    "        input_id = tokenizer_instance.convert_tokens_to_ids(tokens)\n",
    "        input_id += [0] * (max_len - len(input_id))\n",
    "        input_ids_list.append(input_id)\n",
    "\n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(tokens) + [0] * (max_len - len(tokens))\n",
    "\n",
    "        # Pad attention_mask to match max_len (100)\n",
    "        attention_mask += [0] * (max_len - len(attention_mask))\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids_list)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    y_label = torch.tensor(df['label'])\n",
    "\n",
    "\n",
    "    if apply_smote:\n",
    "\n",
    "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        input_ids_resampled, y_resampled = smote.fit_resample(input_ids, y_label)\n",
    "\n",
    "        y_resampled = torch.tensor(y_resampled)\n",
    "        input_ids_resampled = torch.tensor(input_ids_resampled)\n",
    "\n",
    "        # Calculate length of resampled y_labels\n",
    "        max_samples = len(y_resampled)\n",
    "\n",
    "        # Pad attention_masks to match the original max_samples\n",
    "        attention_masks = torch.cat([attention_masks, torch.zeros(max_samples-len(attention_masks), max_len)])\n",
    "\n",
    "        # Cast attention masks to a tensor\n",
    "        padded_attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "        return input_ids_resampled, padded_attention_masks, y_resampled\n",
    "    \n",
    "    if test_data:\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    return input_ids, attention_masks, y_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set batch size\n",
    "# Ex: We have a batch size of 100 with a max length of 100 tokens. 100 tweets with a max of 100 words (tokens) per tensor\n",
    "batch_size = 100\n",
    "\n",
    "# Preprocess training, validation, & testing data\n",
    "input_ids_train, attention_masks_train, y_train = preprocess_data(train_data, XLNetTokenizer, apply_smote=True)\n",
    "input_ids_val, attention_masks_val, y_val = preprocess_data(val_data, XLNetTokenizer)\n",
    "input_ids_test, attention_masks_test = preprocess_data(test_data, XLNetTokenizer, test_data=True)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation data\n",
    "val_data = TensorDataset(input_ids_val, attention_masks_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "# Create DataLoader for test data\n",
    "test_data = TensorDataset(input_ids_test, attention_masks_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the XLNet-based model, num_labels = (1, 0) = 2\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)  \n",
    "\n",
    "# Define optimizer and learning rate: .00002\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c41fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()\n",
    "    print(f\"Found GPU at index {device}: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"No GPU devices found. Make sure your GPU is properly configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee85ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### > Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "# Send model to gpu or cpu\n",
    "model.to(device)\n",
    "\n",
    "# Create a TensorBoard writer so that after model training we can \n",
    "# visualize metrics\n",
    "writer = SummaryWriter(log_dir='logs')\n",
    "\n",
    "''' Training loop:\n",
    " 1. Forward Pass: pass a batch through the model to get predictions\n",
    " 2. Compute Loss: calculate the loss between the true labels and predictions\n",
    " 3. Backward Pass: use the loss to compute gradients for all model params\n",
    "                   w.r.t the loss '''\n",
    "\n",
    "# Start the model training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Set model to 'train mode'\n",
    "    model.train()\n",
    "\n",
    "    # Instantiate loss variable\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop through batches of 100 tweets stored in the training loader\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # Unpack the batch 'loop-variable'\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        # Send unpacked batch's constituent parts to either cpu or gpu\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "        # Reset all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward input_ids, attention_masks, and labels through XLNet model\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "        # Obtain the forward loss from the XLNet model: \n",
    "        # # It represents how well the models predictions match the true labels\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Obtain the backward loss from the XLNet model:\n",
    "        # Is used to calculate the derivatives of the model's parameters\n",
    "        # w.r.t forward loss. Calculated through back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Minimizes the loss, and adjusts the models weights and biases\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulates the total loss for each batch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate the average training loss per epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Log training loss to TensorBoard\n",
    "    writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
    "\n",
    "    \n",
    "    \n",
    "    ### > Validation loop: Monitors progress of training\n",
    "    \n",
    "    # Set model to 'evaluation mode'\n",
    "    model.eval()\n",
    "\n",
    "    # Container for predicted hate-speech or non-hate-speech\n",
    "    y_pred_labels = []\n",
    "\n",
    "    # Actual values for hate-speech or non-hate-speech\n",
    "    y_actual_labels = []\n",
    "\n",
    "    # Set loss value equal to zero\n",
    "    val_loss = 0\n",
    "\n",
    "    # The context manager is used to temporarily disable gradient calcs\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # batch: sample of 100 tweets with 100 tokens\n",
    "        for batch in val_loader:\n",
    "\n",
    "            # Unpack each variable in each batch\n",
    "            input_ids, attention_masks, labels = batch\n",
    "\n",
    "            # Send to the device that is being used: cpu or gpu\n",
    "            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward input_ids, attention_masks, and labels through XLNet model\n",
    "            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Store actual labels of hate-speech or no-hate-speece (0 or 1)\n",
    "            y_actual_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # .extend() takes the labels from y_pred_labels and casts them into a numpy array\n",
    "            y_pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calculate the average validation loss per epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate validation accuracy and print classification report\n",
    "    val_accuracy = accuracy_score(y_actual_labels, y_pred_labels)\n",
    "    val_report = classification_report(y_actual_labels, y_pred_labels)\n",
    "    \n",
    "    # Log validation loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_accuracy, epoch)\n",
    "    \n",
    "    # Print training and validation metrics\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(val_report)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "\n",
    "# Save the trained coefficients of the neural net as soon as validation is done\n",
    "torch.save(model.state_dict(), os.getcwd())\n",
    "\n",
    "\n",
    "### > Testing Loop: unseen data\n",
    "\n",
    "# Puts model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# list for predicted labels\n",
    "y_pred_test = []\n",
    "\n",
    "# List for actual labels\n",
    "y_actual_test = []\n",
    "\n",
    "# Set the loss to zero\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Batch: 100 tweets with 100 tokens in each batch\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # Unpack each variable in each batch\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        # Send constituents to the device being used: cpu or gpu\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward input_ids, attention_masks, and labels through XLNet model\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Accumulate test loss\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Store true labels\n",
    "        y_actual_test.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Get model predictions\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        y_pred_test.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate the average test loss per epoch\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Calculate test accuracy and print classification report\n",
    "test_accuracy = accuracy_score(y_actual_test, y_pred_test)\n",
    "test_report = classification_report(y_actual_test, y_pred_test)\n",
    "\n",
    "# Log test loss and accuracy to TensorBoard\n",
    "writer.add_scalar('Loss/Test', avg_test_loss, epoch)\n",
    "writer.add_scalar('Accuracy/Test', test_accuracy, epoch)\n",
    "\n",
    "# Print test metrics\n",
    "print(\"Test Loss:\", avg_test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803de2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in powershell, then open up localhost:6006 in webbrowser\n",
    "\n",
    " tensorboard --logdir=C:/Users/andre/'Job Portfolio Projects'/DataGlacierVI/sentiment.analysis/logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2ca03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
